ARG BASE_IMAGE
FROM ${BASE_IMAGE}

RUN apt-get update \
	&& DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
		autoconf \
		automake \
		autotools-dev \
		build-essential \
		ca-certificates \
		curl \
		daemontools \
		ibverbs-providers \
		libibverbs1 \
		libkrb5-dev \
		librdmacm1 \
		libssl-dev \
		libtool \
		git \
		krb5-user \
		g++ \
		cmake \
		make \
		openssh-client \
		openssh-server \
		pkg-config \
		wget \
		nfs-common \
                libnuma1 \
                libnuma-dev \
                libpmi2-0-dev \
		unattended-upgrades \
	&& unattended-upgrade \
	&& rm -rf /var/lib/apt/lists/* \
	&& rm /etc/ssh/ssh_host_ecdsa_key \
	&& rm /etc/ssh/ssh_host_ed25519_key \
	&& rm /etc/ssh/ssh_host_rsa_key \
        && ln -s /usr/include/slurm-wlm /usr/include/slurm
		
# Remove the ompi/ucx, etc that is in the base image
RUN rm -rf /opt/hpcx

COPY dockerfile_scripts /tmp/det_dockerfile_scripts

ARG WITH_MPI
ARG WITH_OFI
ARG UCX_INSTALL_DIR=/container/ucx
ARG OMPI_INSTALL_DIR=/container/ompi
ARG OFI_INSTALL_DIR=/container/ofi
ARG OMPI_WITH_CUDA=1
RUN if [ "$WITH_MPI" = "1" ]; then /tmp/det_dockerfile_scripts/ompi.sh "$UBUNTU_VERSION" "$WITH_OFI" "$OMPI_WITH_CUDA"; fi

# Make sure OMPI/UCX show up in the right paths
ARG VERBS_LIB_DIR=/usr/lib/libibverbs
ARG UCX_LIB_DIR=${UCX_INSTALL_DIR}/lib:${UCX_INSTALL_DIR}/lib64
ARG UCX_PATH_DIR=${UCX_INSTALL_DIR}/bin
ARG OFI_LIB_DIR=${OFI_INSTALL_DIR}/lib:${OFI_INSTALL_DIR}/lib64
ARG OFI_PATH_DIR=${OFI_INSTALL_DIR}/bin
ARG OMPI_LIB_DIR=${OMPI_INSTALL_DIR}/lib
ARG OMPI_PATH_DIR=${OMPI_INSTALL_DIR}/bin

# Set up UCX_LIBS and OFI_LIBS
ENV UCX_LIBS="${VERBS_LIB_DIR}:${UCX_LIB_DIR}:${OMPI_LIB_DIR}:"
ENV OFI_LIBS="${VERBS_LIB_DIR}:${OFI_LIB_DIR}:${OMPI_LIB_DIR}:"

# If WITH_OFI is true, then set EXTRA_LIBS to OFI libs, else set to empty string
ENV EXTRA_LIBS="${WITH_OFI:+${OFI_LIBS}}"

# If EXTRA_LIBS is empty, set to UCX libs, else leave as OFI libs
ENV EXTRA_LIBS="${EXTRA_LIBS:-${UCX_LIBS}}"

# But, only add them if WITH_MPI
ENV LD_LIBRARY_PATH=${WITH_MPI:+$EXTRA_LIBS}:$LD_LIBRARY_PATH

#USING OFI
ENV PATH=${WITH_OFI:+${WITH_MPI:+$OFI_PATH_DIR:$OMPI_PATH_DIR}:$PATH}
ENV PATH=/container/ompi/bin:$PATH
ENV LD_LIBRARY_PATH=/container/ompi/lib:$LD_LIBRARY_PATH

# Enable running OMPI as root
ENV OMPI_ALLOW_RUN_AS_ROOT ${WITH_MPI:+1}
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM ${WITH_MPI:+1}

# Need to override this so we don't try using the OMPI built into the
# base container, which is not built correctly for libfabric
ENV OPAL_PREFIX=/container/ompi

# Install Horovod
ENV HOROVOD_NCCL_LINK=${WITH_NCCL:+SHARED}
ARG HOROVOD_PIP=""
ARG HOROVOD_WITH_TENSORFLOW=0
ARG HOROVOD_WITH_PYTORCH=1
ARG HOROVOD_WITHOUT_MXNET=1
ARG HOROVOD_WITH_MPI=1
ARG HOROVOD_CPU_OPERATIONS=MPI
ARG HOROVOD_GPU_OPERATIONS=NCCL
ARG HOROVOD_WITHOUT_MPI=0
ENV HOROVOD_MPI=/container/ompi
RUN if [ -n "${HOROVOD_PIP}" ]; then ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs && \
    pip install "$HOROVOD_PIP" && \
    ldconfig; fi

ARG AWS_PLUGIN_INSTALL_DIR=/container/aws
ARG WITH_AWS_TRACE
ARG INTERNAL_AWS_DS
ARG INTERNAL_AWS_PATH
RUN if [ "$WITH_OFI" = "1" ]; then /tmp/det_dockerfile_scripts/build_aws.sh "$WITH_OFI" "$WITH_AWS_TRACE"; fi

#USING OFI
ARG AWS_LIB_DIR=${AWS_PLUGIN_INSTALL_DIR}/lib
ENV LD_LIBRARY_PATH=${WITH_OFI:+$AWS_LIB_DIR:}$LD_LIBRARY_PATH

# We uninstall these packages after installing. This ensures that we can
# successfully install these packages into containers running as non-root.
# `pip` does not uninstall dependencies, so we still have all the dependencies
# installed.
RUN python -m pip install determined && python -m pip uninstall -y determined

RUN python -m pip install -r /tmp/det_dockerfile_scripts/notebook-requirements.txt
ENV JUPYTER_CONFIG_DIR=/run/determined/jupyter/config
ENV JUPYTER_DATA_DIR=/run/determined/jupyter/data
ENV JUPYTER_RUNTIME_DIR=/run/determined/jupyter/runtime

RUN /tmp/det_dockerfile_scripts/add_det_nobody_user.sh

RUN /tmp/det_dockerfile_scripts/install_libnss_determined.sh

# Set an entrypoint that can scrape up the host libfabric.so and then 
# run the user command. This is intended to enable performant execution
# on non-IB systems that have a proprietary libfabric.
RUN mkdir -p /container/bin && \
    cp /tmp/det_dockerfile_scripts/scrape_libs.sh /container/bin
ENTRYPOINT ["/container/bin/scrape_libs.sh"]
CMD ["/bin/bash"]

RUN /tmp/det_dockerfile_scripts/install_google_cloud_sdk.sh

RUN python -m pip install -r /tmp/det_dockerfile_scripts/additional-requirements.txt

# LL_NOTE: need to figure out why nvidia sets user to 1000 for the ngc container
# that we are building from and whether there are any issues to us not doing the same
# when building this container.
RUN chown root /usr/lib && chgrp root /usr/lib

# This only matter if we are targeting the H100s/GH nodes
# Force it to use a precompiled version of the aws ofi plugin
RUN if [ "$WITH_GH" = "1" ]; then cp ./aws-libs.tar /tmp; fi
RUN if [ "$WITH_GH" = "1" ]; then tar -C /tmp -xf /tmp/aws-libs.tar && \
    mv /tmp/aws-libs/libnccl-net.* /container/aws/lib && \
    chown -R root:root /container/aws && \
    chmod -R go+rX /container/aws; fi

RUN rm -r /tmp/*

