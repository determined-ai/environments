ARG BASE_IMAGE
FROM ${BASE_IMAGE}

RUN mkdir -p /var/run/sshd
RUN apt-get update \
	&& DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
		autoconf \
		automake \
		autotools-dev \
		build-essential \
		ca-certificates \
		curl \
		daemontools \
		ibverbs-providers \
		libibverbs1 \
		libkrb5-dev \
		librdmacm1 \
		libssl-dev \
		libtool \
		git \
		krb5-user \
		g++ \
		cmake \
		make \
		openssh-client \
		openssh-server \
		pkg-config \
		wget \
		nfs-common \
                libnuma1 \
                libnuma-dev \
                libpmi2-0-dev \
		unattended-upgrades \
	&& unattended-upgrade \
	&& rm -rf /var/lib/apt/lists/* \
	&& rm /etc/ssh/ssh_host_ecdsa_key \
	&& rm /etc/ssh/ssh_host_ed25519_key \
	&& rm /etc/ssh/ssh_host_rsa_key \
   && ln -s /usr/include/slurm-wlm /usr/include/slurm
		
COPY dockerfile_scripts /tmp/det_dockerfile_scripts

# Install debuild util, etc. for later compiling GDRcopy libraries
# RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y devscripts debhelper

# Note for the NGC GH image the libgdr is already installed so don't need this
# Install GDRcopy so OMPI, NCCL, AWS plugin, etc, can see it
# RUN /tmp/det_dockerfile_scripts/build_gdrcopy.sh

ARG WITH_MPI
ARG WITH_OFI
ARG UCX_INSTALL_DIR=/container/ucx
ARG OMPI_INSTALL_DIR=/container/ompi
ARG OFI_INSTALL_DIR=/container/ofi
ARG OMPI_WITH_CUDA=1
RUN if [ "$WITH_MPI" = "1" ]; then /tmp/det_dockerfile_scripts/ompi.sh "$UBUNTU_VERSION" "$WITH_OFI" "$OMPI_WITH_CUDA"; fi
# Make sure OMPI/UCX show up in the right paths
ARG VERBS_LIB_DIR=/usr/lib/libibverbs
ARG UCX_LIB_DIR=${UCX_INSTALL_DIR}/lib:${UCX_INSTALL_DIR}/lib64
ARG UCX_PATH_DIR=${UCX_INSTALL_DIR}/bin
ARG OFI_LIB_DIR=${OFI_INSTALL_DIR}/lib:${OFI_INSTALL_DIR}/lib64
ARG OFI_PATH_DIR=${OFI_INSTALL_DIR}/bin
ARG OMPI_LIB_DIR=${OMPI_INSTALL_DIR}/lib
ARG OMPI_PATH_DIR=${OMPI_INSTALL_DIR}/bin

# Set up UCX_LIBS and OFI_LIBS
ENV UCX_LIBS="${VERBS_LIB_DIR}:${UCX_LIB_DIR}:${OMPI_LIB_DIR}:"
ENV OFI_LIBS="${VERBS_LIB_DIR}:${OFI_LIB_DIR}:${OMPI_LIB_DIR}:"

# If WITH_OFI is true, then set EXTRA_LIBS to OFI libs, else set to empty string
ENV EXTRA_LIBS="${WITH_OFI:+${OFI_LIBS}}"

# If EXTRA_LIBS is empty, set to UCX libs, else leave as OFI libs
ENV EXTRA_LIBS="${EXTRA_LIBS:-${UCX_LIBS}}"

# But, only add them if WITH_MPI
ENV LD_LIBRARY_PATH=${WITH_MPI:+$EXTRA_LIBS}$LD_LIBRARY_PATH

#USING OFI
ENV PATH=${WITH_OFI:+$PATH:${WITH_MPI:+$OFI_PATH_DIR:$OMPI_PATH_DIR}}

#USING UCX
ENV PATH=${PATH:-$CONDA:${WITH_MPI:+$UCX_PATH_DIR:$OMPI_PATH_DIR}}

# Enable running OMPI as root
ENV OMPI_ALLOW_RUN_AS_ROOT ${WITH_MPI:+1}
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM ${WITH_MPI:+1}

# Install Horovod
ENV HOROVOD_NCCL_LINK=${WITH_NCCL:+SHARED}
ARG HOROVOD_PIP=""
ARG HOROVOD_WITH_TENSORFLOW=0
ARG HOROVOD_WITH_PYTORCH=1
ARG HOROVOD_WITHOUT_MXNET=1
ARG HOROVOD_WITH_MPI
ARG HOROVOD_CPU_OPERATIONS
ARG HOROVOD_GPU_OPERATIONS
ARG HOROVOD_WITHOUT_MPI
#RUN pip install cmake==3.22.4 protobuf==3.20.3
#RUN pip install "$HOROVOD_PIP" 
#RUN if [ -n "${HOROVOD_PIP}" ]; then ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs && \
#    pip install "$HOROVOD_PIP" && \
#    ldconfig; fi

ARG AWS_PLUGIN_INSTALL_DIR=/container/aws
ARG WITH_AWS_TRACE
ARG INTERNAL_AWS_DS
ARG INTERNAL_AWS_PATH
RUN if [ "$WITH_OFI" = "1" ]; then /tmp/det_dockerfile_scripts/build_aws.sh "$WITH_OFI" "$WITH_AWS_TRACE"; fi
#USING OFI
ARG AWS_LIB_DIR=${AWS_PLUGIN_INSTALL_DIR}/lib
ENV LD_LIBRARY_PATH=${WITH_OFI:+$AWS_LIB_DIR:}$LD_LIBRARY_PATH





# We uninstall these packages after installing. This ensures that we can
# successfully install these packages into containers running as non-root.
# `pip` does not uninstall dependencies, so we still have all the dependencies
# installed.
RUN python -m pip install determined && python -m pip uninstall -y determined

RUN python -m pip install -r /tmp/det_dockerfile_scripts/notebook-requirements.txt
ENV JUPYTER_CONFIG_DIR=/run/determined/jupyter/config
ENV JUPYTER_DATA_DIR=/run/determined/jupyter/data
ENV JUPYTER_RUNTIME_DIR=/run/determined/jupyter/runtime

RUN /tmp/det_dockerfile_scripts/add_det_nobody_user.sh

RUN /tmp/det_dockerfile_scripts/install_libnss_determined.sh

# Set an entrypoint that can scrape up the host libfabric.so and then 
# run the user command. This is intended to enable performant execution
# on non-IB systems that have a proprietary libfabric.
RUN mkdir -p /container/bin && \
    cp /tmp/det_dockerfile_scripts/scrape_libs.sh /container/bin
ENTRYPOINT ["/container/bin/scrape_libs.sh"]
CMD ["/bin/bash"]

RUN /tmp/det_dockerfile_scripts/install_google_cloud_sdk.sh

RUN python -m pip install -r /tmp/det_dockerfile_scripts/additional-requirements.txt

#ARG DEEPSPEED_PIP
#RUN if [ -n "$DEEPSPEED_PIP" ]; then /tmp/det_dockerfile_scripts/install_deepspeed.sh; fi

# Copy Liam's permissions setting code from Lore project
# Make sure permissions are okay for nonroot and cleanup.
RUN chmod -R 777 /usr/local/lib/python3.10/dist-packages/smart* && chmod -R 777 /tmp
# LL_NOTE: need to figure out why nvidia sets user to 1000 for the ngc container
# that we are building from and whether there are any issues to us not doing the same
# when building this container.
RUN chown root /usr/lib && chgrp root /usr/lib

RUN rm -r /tmp/*

ENV PATH=/container/aws/bin:/container/ofi/bin:/container/ompi/bin:${PATH}
ENV LD_LIBRARY_PATH=/container/aws/lib:/container/ofi/lib:/container/ompi/lib:${PATH}
